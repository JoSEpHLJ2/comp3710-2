import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import os
import time
import matplotlib.pyplot as plt

from gan_model import Generator, Discriminator, weights_init
from gan_data import get_dataloader
from gan_config import config

def save_samples_short(generator, epoch, batch_i, fixed_noise):
    """å¿«é€Ÿä¿å­˜æ ·æœ¬"""
    generator.eval()
    with torch.no_grad():
        fake_images = generator(fixed_noise).detach().cpu()
    
    # åªä¿å­˜ä¸€å¼ æ ·æœ¬å›¾
    img = fake_images[0].squeeze().numpy()
    img = (img + 1) / 2  # åå½’ä¸€åŒ–åˆ° [0, 1]
    
    plt.figure(figsize=(4, 4))
    plt.imshow(img, cmap='gray')
    plt.axis('off')
    plt.title(f'Epoch {epoch}, Batch {batch_i}')
    plt.savefig(os.path.join(config.output_dir, "samples", f"sample_epoch{epoch}_batch{batch_i}.png"))
    plt.close()
    
    generator.train()

def train_gan_short():
    print("ğŸš€ å¼€å§‹çŸ­æœŸGANè®­ç»ƒ (3è½®)")
    
    # åˆå§‹åŒ–æ¨¡å‹
    generator = Generator().to(config.device)
    discriminator = Discriminator().to(config.device)
    
    # åˆå§‹åŒ–æƒé‡
    generator.apply(weights_init)
    discriminator.apply(weights_init)
    
    # ä½¿ç”¨æ›´ç¨³å®šçš„è®¾ç½®
    criterion = nn.BCELoss()
    g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))
    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))
    
    # æ•°æ®åŠ è½½å™¨
    dataloader = get_dataloader()
    
    # å›ºå®šå™ªå£°ç”¨äºç”Ÿæˆæ ·æœ¬
    fixed_noise = torch.randn(16, config.latent_dim, 1, 1, device=config.device)
    
    print(f"è®¾å¤‡: {config.device}")
    print(f"æ¯è½®æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
    print(f"æ€»è½®æ•°: {config.epochs}")
    
    for epoch in range(config.epochs):
        start_time = time.time()
        epoch_d_losses = []
        epoch_g_losses = []
        
        print(f"\n=== ç¬¬ {epoch+1}/{config.epochs} è½® ===")
        
        for i, real_images in enumerate(dataloader):
            batch_size = real_images.size(0)
            real_images = real_images.to(config.device)
            
            # ä½¿ç”¨è½¯æ ‡ç­¾
            real_labels = torch.full((batch_size,), 0.9, device=config.device)
            fake_labels = torch.full((batch_size,), 0.1, device=config.device)
            
            # ---------------------
            # è®­ç»ƒåˆ¤åˆ«å™¨
            # ---------------------
            discriminator.zero_grad()
            
            # çœŸå®å›¾åƒ
            outputs_real = discriminator(real_images)
            d_loss_real = criterion(outputs_real, real_labels)
            
            # å‡å›¾åƒ
            noise = torch.randn(batch_size, config.latent_dim, 1, 1, device=config.device)
            fake_images = generator(noise)
            outputs_fake = discriminator(fake_images.detach())
            d_loss_fake = criterion(outputs_fake, fake_labels)
            
            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            d_optimizer.step()
            
            # ---------------------
            # è®­ç»ƒç”Ÿæˆå™¨
            # ---------------------
            generator.zero_grad()
            outputs = discriminator(fake_images)
            g_loss = criterion(outputs, real_labels)
            g_loss.backward()
            g_optimizer.step()
            
            # è®°å½•æŸå¤±
            epoch_d_losses.append(d_loss.item())
            epoch_g_losses.append(g_loss.item())
            
            # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡
            if i % 50 == 0:
                print(f'æ‰¹æ¬¡ [{i}/{len(dataloader)}] DæŸå¤±: {d_loss.item():.4f} GæŸå¤±: {g_loss.item():.4f}')
            
            # æ¯100ä¸ªbatchä¿å­˜æ ·æœ¬
            if i % 100 == 0:
                save_samples_short(generator, epoch+1, i, fixed_noise)
        
        # æ¯è½®ç»“æŸç»Ÿè®¡
        epoch_time = time.time() - start_time
        avg_d_loss = np.mean(epoch_d_losses)
        avg_g_loss = np.mean(epoch_g_losses)
        
        print(f"ç¬¬ {epoch+1} è½®å®Œæˆ, è€—æ—¶: {epoch_time:.2f}s")
        print(f"å¹³å‡DæŸå¤±: {avg_d_loss:.4f}, å¹³å‡GæŸå¤±: {avg_g_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        torch.save(generator.state_dict(), os.path.join(config.output_dir, "checkpoints", f'generator_epoch{epoch+1}.pt'))
        torch.save(discriminator.state_dict(), os.path.join(config.output_dir, "checkpoints", f'discriminator_epoch{epoch+1}.pt'))
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(generator.state_dict(), os.path.join(config.output_dir, 'generator_final.pt'))
    torch.save(discriminator.state_dict(), os.path.join(config.output_dir, 'discriminator_final.pt'))
    
    print("\nâœ… 3è½®è®­ç»ƒå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {config.output_dir}")

if __name__ == "__main__":
    train_gan_short()